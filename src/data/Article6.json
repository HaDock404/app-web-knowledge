{
  "title": "Scraping",
  "content": [
    {
      "type": "image",
      "src": "https://kinsta.com/wp-content/uploads/2022/07/Types-of-web-data.png",
      "alt": ""
    },
    {
      "type": "index",
      "items": [
        { "text": "Introduction", "anchor": "anchor0" },
        { "text": "Les bibliothèques principales", "anchor": "anchor1" },
        { "text": "Scraping d’une page statique avec requests + BeautifulSoup", "anchor": "anchor2" },
        { "text": "Scraping d’une page dynamique avec Selenium", "anchor": "anchor3" },
        { "text": "Exemple complet : scraping des salaires NBA avec Selenium", "anchor": "anchor4" },
        { "text": "Scraping dynamique avec scrolling infini", "anchor": "anchor5" },
        { "text": "Gestion des cookies et des boutons “Accepter”", "anchor": "anchor6" }

        
      ]
    },


    {
      "type": "heading",
      "level": 2,
      "text": "Introduction",
      "anchor": "anchor0"
    },
    {
      "type": "paragraph",
      "text": "Le scraping consiste à extraire des données d’un site web de manière automatisée afin de constituer des jeux de données exploitables (analyses, dashboards, entraînement de modèles, veille concurrentielle, etc.)."
    },


    {
      "type": "heading",
      "level": 2,
      "text": "Les bibliothèques principales",
      "anchor": "anchor1"
    },
    {
      "type": "heading",
      "level": 3,
      "text": "requests"
    },
    {
      "type": "paragraph",
      "text": "- Pour des pages statiques simples (HTML accessible directement)."
    },
    {
      "type": "paragraph",
      "text": "- Rapide, léger, ne charge pas JavaScript."
    },
    {
      "type": "heading",
      "level": 3,
      "text": "BeautifulSoup"
    },
    {
      "type": "paragraph",
      "text": "- Pour parser le HTML récupéré par requests."
    },
    {
      "type": "paragraph",
      "text": "- Extraction par balises, classes, ids."
    },
    {
      "type": "heading",
      "level": 3,
      "text": "Selenium"
    },
    {
      "type": "paragraph",
      "text": "- Pour des pages dynamiques nécessitant l'exécution de JavaScript."
    },
    {
      "type": "paragraph",
      "text": "- Permet d'interagir avec les boutons, scrolling, menus déroulants."
    },


    {
      "type": "heading",
      "level": 2,
      "text": "Scraping d’une page statique avec requests + BeautifulSoup",
      "anchor": "anchor2"
    },
    {
      "type": "code",
      "text": "import requests\nfrom bs4 import BeautifulSoup\nimport pandas as pd\n\nurl = \"https://www.example.com/produits\"\nresponse = requests.get(url)\nsoup = BeautifulSoup(response.text, \"html.parser\")\n\n# Exemple : récupérer les titres des produits\ntitles = [title.text.strip() for title in soup.find_all(\"h2\", class_=\"titre-produit\")]\n\ndf = pd.DataFrame(titles, columns=[\"Titre\"])\ndf.to_csv(\"produits.csv\", index=False)"
    },


    {
      "type": "heading",
      "level": 2,
      "text": "Scraping d’une page dynamique avec Selenium",
      "anchor": "anchor3"
    },
    {
      "type": "paragraph",
      "text": "Installation"
    },
    {
      "type": "code",
      "text": "pip install selenium"
    },
    {
      "type": "paragraph",
      "text": "Télécharger ChromeDriver (ou GeckoDriver pour Firefox) et vérifier sa compatibilité avec la version du navigateur."
    },
    {
      "type": "paragraph",
      "text": "Configuration de base"
    },
    {
      "type": "code",
      "text": "from selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.chrome.service import Service\nfrom selenium.webdriver.chrome.options import Options\n\nchrome_options = Options()\nchrome_options.add_argument(\"--headless\")  # en option pour ne pas ouvrir la fenêtre\ndriver = webdriver.Chrome(service=Service(\"/path/to/chromedriver\"), options=chrome_options)\n\ndriver.get(\"https://www.example.com\")"
    },


    {
      "type": "heading",
      "level": 2,
      "text": "Exemple complet : scraping des salaires NBA avec Selenium",
      "anchor": "anchor4"
    },
    {
      "type": "paragraph",
      "text": "Extraction simple"
    },
    {
      "type": "code",
      "text": "from selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport pandas as pd\n\ndriver.get('https://hoopshype.com/salaries/players/')\n\nWebDriverWait(driver, 10).until(\n    EC.presence_of_element_located((By.XPATH, '//td[@class=\"name\"]'))\n)\nplayers = driver.find_elements(By.XPATH, '//td[@class=\"name\"]')\nsalaries = driver.find_elements(By.XPATH, '//td[@class=\"hh-salaries-sorted\"]')\n\nplayers_list = [p.text for p in players]\nsalaries_list = [s.text for s in salaries]\n\ndf = pd.DataFrame({\"Player\": players_list, \"Salary\": salaries_list})\ndf.to_csv(\"nba_salaries.csv\", index=False)\n\ndriver.quit()"
    },


    {
      "type": "heading",
      "level": 2,
      "text": "Scraping dynamique avec scrolling infini",
      "anchor": "anchor5"
    },
    {
      "type": "paragraph",
      "text": "Certains sites (LinkedIn, JO, e-commerce) nécessitent du scrolling pour charger les données."
    },
    {
      "type": "paragraph",
      "text": "Exemple simplifié :"
    },
    {
      "type": "code",
      "text": "import time\n\ndef scroll_and_collect(driver, scroll_step=300):\n    last_scroll_position = -1\n    while True:\n        driver.execute_script(f\"window.scrollBy(0, {scroll_step});\")\n        time.sleep(1)\n        new_position = driver.execute_script(\"return window.pageYOffset;\")\n        if new_position == last_scroll_position:\n            break\n        last_scroll_position = new_position"
    },


    {
      "type": "heading",
      "level": 2,
      "text": "Gestion des cookies et des boutons “Accepter”",
      "anchor": "anchor6"
    },
    {
      "type": "paragraph",
      "text": "Les sites affichent souvent des pop-ups RGPD :"
    },
    {
      "type": "code",
      "text": "from selenium.webdriver.common.by import By\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\n\ntry:\n    accept_button = WebDriverWait(driver, 10).until(\n        EC.element_to_be_clickable((By.ID, \"onetrust-accept-btn-handler\"))\n    )\n    accept_button.click()\nexcept:\n    pass  # s'il n'existe pas, on continue"
    }
  ]
}
